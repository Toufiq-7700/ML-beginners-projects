import numpy as np

# -----------------------------
# Step 1: Create synthetic dataset (30 patients)
# -----------------------------
# Features: [age, tumor_size, genetic_risk]
X = np.array([
    [25,1,0],[30,2,0],[45,5,1],[50,6,1],[55,7,1],
    [60,8,1],[35,3,0],[40,4,0],[29,2,0],[62,9,1],
    [47,5,1],[53,6,1],[58,7,1],[61,9,1],[39,4,0],
    [42,5,1],[31,2,0],[34,3,0],[48,6,1],[56,7,1],
    [37,4,0],[44,5,1],[59,8,1],[63,10,1],[41,4,0],
    [33,3,0],[38,4,0],[52,7,1],[49,6,1],[46,5,1]
], dtype=float)

# Labels: 0 = no cancer, 1 = cancer
y = np.array([0,0,1,1,1,
              1,0,0,0,1,
              1,1,1,1,0,
              1,0,0,1,1,
              0,1,1,1,0,
              0,0,1,1,1], dtype=float)

# Normalize features (helps training)
X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)

# -----------------------------
# Step 2: Define helper functions
# -----------------------------
def sigmoid(z):
    """Sigmoid activation function"""
    return 1 / (1 + np.exp(-z))

def compute_cost(X, y, w, b):
    """Compute logistic regression cost (log loss)"""
    m = X.shape[0]
    z = np.dot(X, w) + b
    f_wb = sigmoid(z)
    cost = - (1/m) * np.sum(y*np.log(f_wb+1e-9) + (1-y)*np.log(1-f_wb+1e-9))
    return cost

def compute_gradient(X, y, w, b):
    """Compute gradients for logistic regression"""
    m, n = X.shape
    z = np.dot(X, w) + b
    f_wb = sigmoid(z)
    error = f_wb - y
    dj_dw = (1/m) * np.dot(X.T, error)
    dj_db = (1/m) * np.sum(error)
    return dj_dw, dj_db

# -----------------------------
# Step 3: Gradient descent
# -----------------------------
def gradient_descent(X, y, w, b, alpha, iterations):
    cost_history = []
    for i in range(iterations):
        dj_dw, dj_db = compute_gradient(X, y, w, b)
        w -= alpha * dj_dw
        b -= alpha * dj_db
        if i % 100 == 0:
            cost = compute_cost(X, y, w, b)
            cost_history.append(cost)
            print(f"Iteration {i}: Cost = {cost:.4f}")
    return w, b, cost_history

# -----------------------------
# Step 4: Train the model
# -----------------------------
m, n = X.shape
w = np.zeros(n)
b = 0
alpha = 0.1
iterations = 1000

w, b, cost_history = gradient_descent(X, y, w, b, alpha, iterations)

print("\nTrained weights:", w)
print("Trained bias:", b)

# -----------------------------
# Step 5: Predictions
# -----------------------------
def predict(X, w, b):
    return sigmoid(np.dot(X, w) + b) >= 0.5

# Accuracy on training set
y_pred = predict(X, w, b)
accuracy = np.mean(y_pred == y)
print("\nTraining Accuracy:", accuracy)

# -----------------------------
# Step 6: Test on new patient
# -----------------------------
new_patient = np.array([45, 6, 1], dtype=float)
# normalize using same mean & std
mean = np.mean(X, axis=0)
std = np.std(X, axis=0)
new_patient_norm = (new_patient - mean) / std

prob = sigmoid(np.dot(new_patient_norm, w) + b)
print("\nNew patient (age=45, tumor=6, risk=1):")
print("Probability of cancer:", prob)
print("Prediction:", "Has cancer" if prob >= 0.5 else "No cancer")
